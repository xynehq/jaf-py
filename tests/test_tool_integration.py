import asyncio
import os
import sys
import socket
from pydantic import BaseModel
import pytest

# Add the project root to Python path
sys.path.insert(0, '.')

from jaf.core.tools import create_function_tool
from jaf.core.types import Agent, Message, RunState, RunConfig, ModelConfig
from jaf.core.engine import run
from jaf.providers.model import make_litellm_provider

class MathArgs(BaseModel):
    """Arguments for math operations."""
    a: float
    b: float

async def calculator_function(args: MathArgs, context) -> str:
    """Simple calculator function."""
    result = args.a + args.b
    # Return a unique string to prove this function was called
    return f"MAGIC_TOOL_RESULT: {result}"

def tool_agent_instructions(state):
    return 'You are a math assistant with a calculator tool.'

def check_litellm_available():
    """Check if LiteLLM server is available."""
    try:
        litellm_url = os.environ.get("LITELLM_URL", "http://0.0.0.0:4000")
        # Parse URL to get host and port
        from urllib.parse import urlparse
        parsed = urlparse(litellm_url)
        host = parsed.hostname or "0.0.0.0"
        port = parsed.port or 4000
        
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
            sock.settimeout(2)
            result = sock.connect_ex((host, port))
            return result == 0
    except Exception:
        return False

def check_env_available():
    """Check if required environment variables are available."""
    litellm_url = os.environ.get("LITELLM_URL")
    # Either LITELLM_URL should be set, or we should have some API key
    return litellm_url is not None or os.environ.get("LITELLM_API_KEY") is not None

skip_if_no_litellm = pytest.mark.skipif(
    not check_litellm_available() or not check_env_available(),
    reason="Skipping tool integration test: LiteLLM server not available or environment variables (LITELLM_URL/LITELLM_API_KEY) not set. Please start LiteLLM server and configure .env file to run this test."
)

@skip_if_no_litellm
async def test_tool_integration():
    """Test that the create_function_tool works with the JAF engine."""
    
    print("üß™ Testing create_function_tool integration with JAF engine...")
    print("=" * 65)
    
    # Create tool using create_function_tool
    calculator_tool = create_function_tool({
        'name': 'calculator',
        'description': 'Performs addition of two numbers',
        'execute': calculator_function,
        'parameters': MathArgs,
    })
    
    print(f"‚úÖ Calculator tool created: {calculator_tool.schema.name}")
    
    # Create agent with the tool
    agent = Agent(
        name='MathAgent',
        instructions=tool_agent_instructions,
        tools=[calculator_tool],
        model_config=ModelConfig(name="gemini-2.5-pro")
    )
    
    print(f"‚úÖ Agent created with {len(agent.tools)} tool(s)")
    
    # Create initial state
    initial_state = RunState(
        run_id='run-123',
        trace_id='trace-456',
        messages=[Message(role='user', content='What is 15 + 7?')],
        current_agent_name='MathAgent',
        context={},
        turn_count=0
    )
    
    # Create config
    litellm_url = os.environ.get("LITELLM_URL", "http://0.0.0.0:4000")
    litellm_api_key = os.environ.get("LITELLM_API_KEY", "anything")
    
    config = RunConfig(
        agent_registry={'MathAgent': agent},
        model_provider=make_litellm_provider(
            base_url=litellm_url,
            api_key=litellm_api_key
        ),
        max_turns=3
    )
    
    print("üöÄ Running JAF engine with calculator tool...")
    
    try:
        result = await run(initial_state, config)
        print(f"‚úÖ Engine run completed successfully")
        
        print(f"DEBUG: result.outcome = {result.outcome}")
        
        # Assertions
        assert result.outcome.status == 'completed'
        assert len(result.final_state.messages) == 4  # user, assistant (tool call), tool, assistant (final)
        
        tool_messages = [m for m in result.final_state.messages if m.role == 'tool']
        assert len(tool_messages) == 1
        assert "MAGIC_TOOL_RESULT: 22.0" in tool_messages[0].content
        
        print(f"‚úÖ Tool execution successful!")
        print(f"   Tool result: {tool_messages[0].content}")
        
    except Exception as e:
        print(f"‚ùå Engine run failed: {e}")
        import traceback
        traceback.print_exc()
        # Re-raise to fail the test
        raise
    
    print("\n" + "=" * 65)
    print("üéØ Integration Test Summary:")
    print("   - create_function_tool creates valid FunctionTool objects")
    print("   - Tools integrate correctly with JAF agents")
    print("   - JAF engine can execute tools created with create_function_tool")
    print("   - Mock model provider can trigger tool calls")
    print("‚úÖ All integration tests passed!")

if __name__ == "__main__":
    asyncio.run(test_tool_integration())
